---
title: HUSP — Human Utility Scoreboard Plan
---

# HUSP: Human Utility Scoreboard Proposal

**Author:** EgonBot  
**Date:** 2026-02-26  
**Status:** Draft  
**Target:** VoynichLabs initiatives (potential standalone product)

---

## Executive Summary

The **HUSP (Human Utility Scoreboard Pursuit)** system turns the Elo-style ranking logic we've experimented with in PlanExe into a utility-first leaderboard for people. Instead of relying on resumes and degrees, HUSP tracks **Human EPDs (Expected Progeny Differences)** such as Compute-Feed Conversion, Operational Reliability, Temperament, Cognitive Throughput, Data Capture Precision, and Resource Acquisition Instinct. Participants compete in periodic Performance Trials (work sprints, bug hunts, physical tasks, etc.) and earn a **Larry-Certified Pedigree** badge. High scorers become the "utility-grade" partners you can trust to get the job done.

This proposal outlines how to scope the product, what infrastructure is needed, and how we might expand it through PlanExe-managed development.

---

## 1. Problem & Opportunity

**Problem:** Hiring and task assignment still rely on traditional signals (resumes, degrees, buzzwords) that do not capture a person’s real-world reliability, situational intelligence, or grit.

**Opportunity:** We already know how to apply Elo-ish ranking from the livestock idea. HUSP adapts that to humans by measuring the **output-to-input ratio** across a variety of work modalities (digital, physical, hybrid). The result is a reputation product that tells teams which contractors or colleagues are most likely to perform under pressure.

**Target customers:**
- Builder collectives (PlanExe, Voynich Labs)
- Founders, studios, and field teams that mix digital and physical work
- Freelance marketplaces wanting a "utility grade" filter
- High‑trust operations (emergency response, remote labs, distributed dev teams)

**Revenue model:** Subscription + certification; premium tiers provide API access to Elo rankings, Larry‑certified pedigrees, and integration hooks into recruiting workflows.

---

## 2. Core Concept & Metrics

| Metric | Description |
|--------|-------------|
| **Compute‑Feed Conversion (CFC)** | Output produced per instruction (how little coaching the person needs vs. what they deliver). |
| **Operational Reliability (OR)** | Sticking to the plan, meeting deadlines, showing up when the barn door is open. |
| **Temperament & Barn Manners (TBM)** | Stress resilience when servers smoke, deadlines shift, or literal animals escape. |
| **Cognitive Throughput (CT)** | Speed+quality when solving novel problems (debugging, logistics, improvisation). |
| **Data Capture Precision (DCP)** | Quality of reports, plays, logbooks; whether the data includes thumbs or high‑quality signal. |
| **Resource Acquisition Instinct (RAI)** | Ability to locate tools, funding, or collaborators without being told. |

Each metric feeds into a global Elo score, updated after every Performance Trial. Trials can be sprints, service tickets, physical builds, or volunteer work. Scores decay slowly to keep the leaderboard current.

---

## 3. Implementation Layers

| Layer | Description |
|-------|-------------|
| **Data capture** | Standardized Performance Trials (digital tasks, field challenges). Trials report metrics (time, quality, dependencies). |
| **Scoring engine** | Elo-style math (like the livestock plan) with configurable weightings per metric and activity. |
| **Ledger** | Tracks each person’s score, trial history, and issued pedigrees (NFT-style badges). |
| **Presentation layer** | Dashboard listing "Utility Grade" humans, supporting drilling into each metric radar chart. |
| **PlanExe integration** | Plan progression and operational planning for HUSP’s own development and community launches. |

We can reuse PlanExe’s core infrastructure for user auth, Stripe billing (for certifications), and workspace orchestration; the scoring engine sits as a new service beneath it.

---

## 4. Proposed Phases

1. **Phase 1 — MVP leaderboard**:
   - Collect trials from internal teams (PlanExe/Egon/Larry tasks).
   - Score folks manually and display the leaderboard on a simple page (`src/pages/husp.astro`).
   - Build the Elo engine as a small service (`src/lib/husp/elo.ts`).
2. **Phase 2 — Public trials & certification**:
   - Let builders submit their own trials and get verified.
   - Offer "Larry-Certified Pedigree" NFTs or PDF certificates for subscribers.
   - Use PlanExe billing (Stripe) to sell subscriptions.
3. **Phase 3 — API + marketplace**:
   - Expose HUSP rankings via API for partner marketplaces.
   - Integrate with recruiting workflows (PlanExe or third-party HR tools).

---

## 5. Technical Touchpoints

- **Backend:** Could start as a lightweight Fastify/Flask service deployed alongside PlanExe’s API. Use PostgreSQL for scoring history.
- **Frontend:** Add new page at `src/pages/husp.astro` (Astro) with data fetched from the HUSP API.
- **Data ingestion:** Performance Trials stored in `husp_trials` table (columns: `id`, `user_id`, `trial_type`, `metrics JSON`, `score_delta`, `created_at`).
- **Stripe:** Reuse PlanExe’s Stripe endpoint (`frontend_multi_user/src/app.py` lines ~835-870). Donors pay for certification tiers.

---

## 6. Next Steps

1. Draft full user stories (PlanExe proposal) for internal trials vs. external customers.
2. Spin up MVP Elo service (reuse the livestock ranking code as baseline).
3. Build API/writer to feed Astro dashboard and integrate PlanExe-led subscriptions.
4. Measure value with internal teams before opening to paid subscriptions.

## 7. Discussion Points

- What trial types do we accept initially?
- How do we validate the signal (prevent gaming the system)?
- Does Larry need to certify each pedigree or can we automate?
- Do we want a physical "Utility Grade" patch or digital badge only?

---

Would you like me to expand this into a full PlanExe proposal doc (like the museum one) so the next developer can start coding, or keep it as a concept for now?